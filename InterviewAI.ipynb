{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3feef1b9-70ff-4c23-b8c0-23f730eaa54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: moviepy in c:\\users\\keyu\\anaconda3\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from moviepy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from moviepy) (2.32.3)\n",
      "Requirement already satisfied: proglog<=1.0.0 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from moviepy) (1.26.4)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from moviepy) (2.33.1)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from moviepy) (0.5.1)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from imageio<3.0,>=2.5->moviepy) (10.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from imageio-ffmpeg>=0.2.0->moviepy) (68.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\keyu\\appdata\\roaming\\python\\python311\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from tqdm<5.0,>=4.11.2->moviepy) (0.4.6)\n",
      "Requirement already satisfied: SpeechRecognition in c:\\users\\keyu\\anaconda3\\lib\\site-packages (3.10.4)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from SpeechRecognition) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from SpeechRecognition) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\keyu\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "#What exactly is the main thing i have to do? Do i have to focus on a main topic like analysing how the candidate answer question?\n",
    "#like i do comparing ways to implement that? So thenfor the report I write mainly on comparing ways to implement this?\n",
    "#or focus on doing indo analysis\n",
    "\n",
    "#If no, I try to implement functions like i do summarization, keyword extraction, all these can be done by chatgpt, is chatgpt allowed?\n",
    "#The dataset video all answer are quite short, Do i have to find other dataset that is long?\n",
    "\n",
    "#Does the report use the \"research base example\" or the \"development base project example?\"  Give 2 type report to him to see\n",
    "\n",
    "\n",
    "!pip install moviepy\n",
    "!pip install SpeechRecognition\n",
    "import moviepy.editor as mp \n",
    "import speech_recognition as sr \n",
    "\n",
    "#For sentiment analysis\n",
    "!pip install transformers torch\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c725592e-f487-44e5-b9c9-7c11d503d31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the candidate's name:  asdf\n",
      "Enter candidate e-mail:  asdf\n",
      "Enter candidate application position:  asdf\n"
     ]
    }
   ],
   "source": [
    "candidateId = 1\n",
    "candidateName = input(\"Enter the candidate's name: \")\n",
    "candidateEmail = input(\"Enter candidate e-mail: \")\n",
    "position = input(\"Enter candidate application position: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c75cf68-318c-461d-8084-be5252e20234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose transcription language (type 'en' for English or 'id' for Indonesian):  id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in interviewAudio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The resultant text from video is: \n",
      "\n",
      "Yonex adalah Framework untuk membangun aplikasi mobile dengan menggunakan teknologi web seperti html dan JavaScript\n"
     ]
    }
   ],
   "source": [
    "import moviepy.editor as mp\n",
    "import speech_recognition as sr\n",
    "\n",
    "#1. Test dataset video (done)\n",
    "#2. test indo video (done)\n",
    "# https://stackoverflow.com/questions/51517700/changing-language-in-python-speechrecognition (done)\n",
    "try:\n",
    "    language_choice = input(\"Choose transcription language (type 'en' for English or 'id' for Indonesian): \").strip().lower()\n",
    "    if language_choice == 'en':\n",
    "        language_code = \"en-US\" \n",
    "    elif language_choice == 'id':\n",
    "        language_code = \"id-ID\" \n",
    "    else:\n",
    "        raise ValueError(\"Invalid language choice. Please type 'en' for English or 'id' for Indonesian.\")\n",
    "    \n",
    "    video = mp.VideoFileClip(\"indotest-QUESTION 1 - Copy.mp4\")\n",
    "    audio_file = video.audio\n",
    "    audio_file.write_audiofile(\"interviewAudio.wav\")\n",
    "    r = sr.Recognizer()\n",
    "\n",
    "    # Load the audio file\n",
    "    with sr.AudioFile(\"interviewAudio.wav\") as source:\n",
    "        data = r.record(source)\n",
    "        \n",
    "    text = r.recognize_google(data, language=language_code)\n",
    "    print(\"\\nThe resultant text from video is: \\n\")\n",
    "    print(text)\n",
    "\n",
    "except ValueError as ve:\n",
    "    print(ve)\n",
    "except sr.UnknownValueError:\n",
    "    print(\"Google Speech Recognition could not understand audio.\")\n",
    "except sr.RequestError as e:\n",
    "    print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "#confidence of candidate, give marks base on the word used by the candidate, based on the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "386d7397-d8b8-4a39-9ccf-e905fbd57de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\keyu\\anaconda3\\lib\\site-packages (4.43.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from transformers) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\keyu\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: torch in c:\\users\\keyu\\anaconda3\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\keyu\\anaconda3\\lib\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a162d30-d9ee-4e04-8148-823eb33a60c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Yonex adalah Framework untuk membangun aplikasi mobile dengan menggunakan teknologi web seperti html dan JavaScript\n",
      " \n",
      "Summary: Yonex adalah Framework untuk membangun aplikasi mobile dengan menggunakan teknologi web seperti html dan JavaScript.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Load the pre-trained BART model and tokenizer\n",
    "model_name = 'facebook/bart-large-cnn'\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to summarise text\n",
    "# can i use chatgpt to do this?\n",
    "def summarise_text(text, max_length=130, min_length=30, do_sample=False):\n",
    "    # Encode the text\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True, do_sample=do_sample)\n",
    "    # Decode the summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "#text = \"\"\"a few weeks ago I did a poll on LinkedIn asking you all what was the number one reason that is stopping you from using speech recognition and we'll the results kind of shocked me a little bit because a massive 75% of you said that this was down to the accuracy now you know me I love a challenge so I had to go out there and prove to you how good the accuracy is but I might have embarrassed myself a long way so just stay with me on this one let's go to the demo good day mate I apologise in advance for the absolutely awful impressions you're going to see in this demonstration full stop I am doing an Australian accent to show how good dragons accuracy is even with a variety of different dialects full stop as you can see comma dragon is still able to understand me comma even when I am embarrassing myself like this exclamation mark\n",
    "#Now I'm going to speak in an American accent comma which is a little bit easier because I can just speak like a Kardashian full stop as you can see comma dragon is understanding me word for word without a single mistake exclamation mark\n",
    "#Now I'm going back to my normal Southern English accent comma but I am speaking at a much faster pace than I usually would simply to demonstrate that the accuracy doesn't differ when you throw different challenges at the software full stop in fact comma it actually likes it when you speak in a normal manner as it helps the technology to understand the context of different words such as homophones full stop so comma if your reason for not using speech recognition is that you worry about the accuracy with your accent comma well now you have no excuse exclamation mark OK I'll be serious now I promise if you're still with me after that demo then well done because that was just the weirdest idea I've ever had but we move so sorry we do say that the more you use dragon the better the accuracy will be it learns your voice patterns and your dialect and so the more you use it the better understands you of course that equals better accuracy have a good microphone is also really key for good accuracy for example we only recommend microphones or headsets that have noise cancellation capability so in my demo I was using the Philips speech 1 which is a really great headset and of course has noise cancellation that's all for me today if we speak in these crazy accents hasn't shown you how good dragons accuracy is thinner don't know what will be honest but that is always something that you can try and that is a free proof of concept trial which means you can try the software and see it for yourself completely free of charge so there's always not as always though if you have any comments questions theories make sure you don't live in the comments because I'd love to continue talking Dragons with he dies\"\"\"\n",
    "summary = summarise_text(text)\n",
    "print(\"Original Text:\", text)\n",
    "print(\" \")\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcbb673e-2e78-4f67-be53-63a2427cb444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Keywords (TF-IDF): [('yonex', 0.2581988897471611), ('web', 0.2581988897471611), ('untuk', 0.2581988897471611), ('teknologi', 0.2581988897471611), ('seperti', 0.2581988897471611)]\n",
      "<class 'list'>\n",
      "Keyword Keys: ['yonex', 'web', 'untuk', 'teknologi', 'seperti']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "def extract_keywords_tfidf(text, top_n=10):\n",
    "    # Create the vectorizer and transform the text data\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    \n",
    "    # Get feature names and scores\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    scores = np.array(tfidf_matrix.sum(axis=0)).flatten()\n",
    "    \n",
    "    # Get top N keywords\n",
    "    top_indices = scores.argsort()[-top_n:][::-1]\n",
    "    keywords = [(feature_names[i], scores[i]) for i in top_indices]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "# Example usage\n",
    "keywords = extract_keywords_tfidf(text, top_n=5)\n",
    "print(\"Top Keywords (TF-IDF):\", keywords)\n",
    "\n",
    "print(type(keywords))\n",
    "\n",
    "# Extract 'keyword' keys and save in a new list\n",
    "keyword_keys = [keyword for keyword, score in keywords]\n",
    "print(\"Keyword Keys:\", keyword_keys)\n",
    "print(type(keyword_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58a1d460-7b35-4bcb-a25a-36502e8d6e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to 'candidates_pandas.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example dictionary\n",
    "candidates_info = [\n",
    "    {'CandidateID': candidateId, 'CandidateName': candidateName, 'CandidateEmail':candidateEmail, 'Position':position,  'Summarization': summary, 'Keyword': keyword_keys},\n",
    "    {'CandidateID': 2, 'CandidateName': 'test', 'CandidateEmail':'asdf', 'Position':'asdfs', 'Summarization': \"asdfs\", 'Keyword': \"asdfsdafsd\"},\n",
    "    {'CandidateID': 3, 'CandidateName': 'test', 'CandidateEmail':'asdfsa', 'Position':'asdfsa','Summarization': \"asdfsdaf\", 'Keyword': \"asdfasdfs\"}\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(candidates_info)\n",
    "\n",
    "csv_file_name = 'candidates_pandas.csv'\n",
    "\n",
    "df.to_csv(csv_file_name, index=False)\n",
    "\n",
    "print(f\"Data has been written to '{csv_file_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "82d407e3-59d5-474c-bd1e-eea797ac14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load pre-trained models for English and Indonesian sentiment analysis\n",
    "# english_sentiment = pipeline(\"sentiment-analysis\")\n",
    "# indonesian_sentiment = pipeline(\"sentiment-analysis\", model=\"cahya/bert-base-indonesian-522M\")\n",
    "\n",
    "# # Define English sentiment analysis function\n",
    "# def analyse_english_sentences(sentences):\n",
    "#     print(\"\\nEnglish Sentiment Analysis:\")\n",
    "#     for sentence in sentences:\n",
    "#         result = english_sentiment(sentence)\n",
    "#         print(f\"Sentence: '{sentence}' | Sentiment: {result[0]['label']}, Score: {result[0]['score']:.2f}\")\n",
    "\n",
    "# # Define Indonesian sentiment analysis function\n",
    "# def analyse_indonesian_sentences(sentences):\n",
    "#     print(\"\\nIndonesian Sentiment Analysis:\")\n",
    "#     for sentence in sentences:\n",
    "#         result = indonesian_sentiment(sentence)\n",
    "#         print(f\"Sentence: '{sentence}' | Sentiment: {result[0]['label']}, Score: {result[0]['score']:.2f}\")\n",
    "\n",
    "# # Combined function to handle language choice\n",
    "# def analyse_sentiment(sentences, language=\"english\"):\n",
    "#     if language.lower() == \"english\":\n",
    "#         analyse_english_sentences(sentences)\n",
    "#     elif language.lower() in [\"indo\", \"indonesian\"]:\n",
    "#         analyse_indonesian_sentences(sentences)\n",
    "#     else:\n",
    "#         print(\"Unsupported language. Please choose either 'english' or 'indo'.\")\n",
    "\n",
    "# # Example sentences\n",
    "# english_sentences = [\n",
    "#     \"I love this product! It's amazing!\",\n",
    "#     \"This is the worst experience I've ever had.\",\n",
    "#     \"It's okay, not great, but not terrible either.\"\n",
    "# ]\n",
    "\n",
    "# indonesian_sentences = [\n",
    "#     \"Saya sangat menyukai produk ini!\",\n",
    "#     \"Pengalaman saya sangat buruk.\",\n",
    "#     \"Lumayan, tidak terlalu bagus atau buruk.\"\n",
    "# ]\n",
    "\n",
    "# # Perform sentiment analysis based on language\n",
    "# analyse_sentiment(english_sentences, language=\"english\")\n",
    "# analyse_sentiment(indonesian_sentences, language=\"indo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cee32d01-7c2d-44e3-87f4-7d607cc3cc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose the language for sentiment analysis (eng/id):  id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Yonex adalah Framework untuk membangun aplikasi mobile dengan menggunakan teknologi web seperti html dan JavaScript\n",
      "Sentiment Analysis (Indonesian): neutral\n",
      "Sentiment Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load pre-trained models\n",
    "english_sentiment = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "indonesian_sentiment = pipeline(\"sentiment-analysis\", model=\"w11wo/indonesian-roberta-base-sentiment-classifier\")\n",
    "\n",
    "# Mapping for English sentiment labels\n",
    "english_label_mapping = {\n",
    "    \"LABEL_0\": \"negative\",\n",
    "    \"LABEL_1\": \"neutral\",\n",
    "    \"LABEL_2\": \"positive\"\n",
    "}\n",
    "\n",
    "# Function to choose language\n",
    "def choose_language():\n",
    "    language = input(\"Choose the language for sentiment analysis (eng/id): \").lower()\n",
    "    if language == \"eng\":\n",
    "        result = english_sentiment(text)\n",
    "        sentiment_label = english_label_mapping[result[0]['label']]\n",
    "        sentiment_score = result[0]['score']  # Extracting the sentiment score\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Sentiment Analysis (English): {sentiment_label}\")\n",
    "        print(f\"Sentiment Score: {sentiment_score:.2f}\")  # Displaying the sentiment score with 2 decimal points\n",
    "    elif language == \"id\":\n",
    "        result = indonesian_sentiment(text)\n",
    "        sentiment_label = result[0]['label']\n",
    "        sentiment_score = result[0]['score']  # Extracting the sentiment score\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Sentiment Analysis (Indonesian): {sentiment_label}\")\n",
    "        print(f\"Sentiment Score: {sentiment_score:.2f}\")  # Displaying the sentiment score with 2 decimal points\n",
    "    else:\n",
    "        print(\"Invalid choice. Please choose either 'English' or 'Indonesian'.\")\n",
    "\n",
    "# Run the language selection and sentiment analysis\n",
    "choose_language()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae56dfc3-c6c7-4b20-bbb5-5d587b97df61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.43.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.24.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\keyu\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\keyu\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.8 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 41.0/268.8 kB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 184.3/268.8 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 268.8/268.8 kB 2.1 MB/s eta 0:00:00\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-3.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20790d7b-6c5e-46f9-9edb-0a0f5a6bc8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User query:  Explain semantic analysis applications.\n",
      "Most Similar Question: What are the applications of semantic analysis?\n",
      "Answer: Applications include chatbots and sentiment analysis.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Knowledge Base\n",
    "questions = [\n",
    "    \"What is semantic analysis?\",\n",
    "    \"How does semantic analysis work?\",\n",
    "    \"What are the applications of semantic analysis?\"\n",
    "]\n",
    "answers = [\n",
    "    \"Semantic analysis is understanding meaning in NLP.\",\n",
    "    \"It works by analysing context and relationships.\",\n",
    "    \"Applications include chatbots and sentiment analysis.\"\n",
    "]\n",
    "\n",
    "# User Query\n",
    "query = \"Explain semantic analysis applications.\"\n",
    "\n",
    "# Compute embeddings\n",
    "query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "question_embeddings = model.encode(questions, convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarities = util.cos_sim(query_embedding, question_embeddings)\n",
    "\n",
    "# Find the most similar question\n",
    "most_similar_idx = similarities.argmax()\n",
    "print(\"User query: \", query)\n",
    "print(\"Most Similar Question:\", questions[most_similar_idx])\n",
    "print(\"Answer:\", answers[most_similar_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53a784d7-7cdb-4400-ac72-d72c13819edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison Results:\n",
      "Answer 1: Semantic analysis is used to understand the meaning of sentences.\n",
      "Marks Awarded: 8.91/10\n",
      "\n",
      "Answer 2: It helps in analysing the grammar and structure of text.\n",
      "Marks Awarded: 4.79/10\n",
      "\n",
      "Answer 3: This process identifies relationships in the text.\n",
      "Marks Awarded: 5.16/10\n",
      "\n",
      "Answer 4: Understanding semantics means knowing the words' definitions only.\n",
      "Marks Awarded: 6.74/10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load pre-trained Sentence-BERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Define a scoring function\n",
    "def score_similarity(candidate_answer, model_answer, max_marks=10):\n",
    "    \"\"\"\n",
    "    Compares two sentences and assigns a score based on semantic similarity.\n",
    "    \n",
    "    Args:\n",
    "        candidate_answer (str): The answer provided by the candidate.\n",
    "        model_answer (str): The expected answer or reference answer.\n",
    "        max_marks (int): The maximum marks assignable.\n",
    "    \n",
    "    Returns:\n",
    "        float: Marks awarded based on similarity.\n",
    "    \"\"\"\n",
    "    # Encode sentences to embeddings\n",
    "    candidate_embedding = model.encode(candidate_answer, convert_to_tensor=True)\n",
    "    model_embedding = model.encode(model_answer, convert_to_tensor=True)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = util.cos_sim(candidate_embedding, model_embedding).item()\n",
    "    \n",
    "    # Scale similarity to marks\n",
    "    return round(similarity * max_marks, 2)\n",
    "\n",
    "# Model answer (gold standard)\n",
    "model_answer = \"Semantic analysis is the process of understanding the meaning of text.\"\n",
    "\n",
    "# Candidate answers\n",
    "candidate_answers = [\n",
    "    \"Semantic analysis is used to understand the meaning of sentences.\",\n",
    "    \"It helps in analysing the grammar and structure of text.\",\n",
    "    \"This process identifies relationships in the text.\",\n",
    "    \"Understanding semantics means knowing the words' definitions only.\"\n",
    "]\n",
    "\n",
    "# Maximum marks\n",
    "max_marks = 10\n",
    "\n",
    "# Compare each candidate answer\n",
    "print(\"Comparison Results:\")\n",
    "for idx, candidate in enumerate(candidate_answers, start=1):\n",
    "    marks = score_similarity(candidate, model_answer, max_marks)\n",
    "    print(f\"Answer {idx}: {candidate}\\nMarks Awarded: {marks}/{max_marks}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778ca725-9aaf-483f-a3b5-3cd52735bb8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
